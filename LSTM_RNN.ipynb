{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pd.read_csv('training_set.csv')\n",
    "metadata_train = pd.read_csv('training_set_metadata.csv')\n",
    "\n",
    "simple_features = train_series.groupby(\n",
    "    ['object_id', 'passband'])['flux'].agg(\n",
    "    ['mean', 'median', 'max', 'min', 'std']).unstack('passband')\n",
    "\n",
    "\n",
    "#construct time series using binned observations:\n",
    "ts_mod = train_series[['object_id', 'mjd', 'passband', 'flux']].copy()\n",
    "#bin by 5 days, reducing the size of data but still giving a time series\n",
    "ts_mod['mjd_d5'] = (ts_mod['mjd'] / 5).astype(int)\n",
    "ts_mod = ts_mod.groupby(['object_id', 'mjd_d5', 'passband'])['flux'].mean().reset_index()\n",
    "\n",
    "#pivotting\n",
    "ts_piv = pd.pivot_table(ts_mod, \n",
    "                        index='object_id', \n",
    "                        columns=['mjd_d5', 'passband'], \n",
    "                        values=['flux'],\n",
    "                        dropna=False)\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del metadata_train['ra'],metadata_train['decl'],metadata_train['gal_l'], metadata_train['gal_b'],metadata_train['hostgal_photoz'],metadata_train['hostgal_photoz_err'], metadata_train['distmod'], metadata_train['mwebv']\n",
    "#Bin into ddf and non-ddf training\n",
    "ddf = metadata_train[(metadata_train['ddf'] == 1)]\n",
    "del ddf['ddf']\n",
    "\n",
    "ddf_far_away= (ddf[(ddf['hostgal_specz'] > 0)])\n",
    "ddf_far_away.set_index('object_id', inplace=True)\n",
    "ddf_nearby= ddf[(ddf['hostgal_specz'] <=0)]\n",
    "ddf_nearby.set_index('object_id', inplace=True)\n",
    "non_ddf = metadata_train[(metadata_train['ddf'] == 0)]\n",
    "del non_ddf['ddf']\n",
    "\n",
    "non_ddf_far_away= non_ddf[(non_ddf['hostgal_specz'] >0)]\n",
    "non_ddf_far_away.set_index('object_id', inplace=True)\n",
    "non_ddf_nearby= non_ddf[(non_ddf['hostgal_specz'] <=0 )]\n",
    "non_ddf_nearby.set_index('object_id', inplace=True)\n",
    "del ddf, non_ddf, ddf_far_away['hostgal_specz'], non_ddf_far_away['hostgal_specz'], ddf_nearby['hostgal_specz'], non_ddf_nearby['hostgal_specz']\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "bins = [ddf_far_away, ddf_nearby, non_ddf_far_away, non_ddf_nearby]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_point(object_id, bin_name):\n",
    "    x = torch.tensor(ts_piv.loc[object_id].values.reshape(-1, 1, 6), dtype = torch.float32)\n",
    "    x[x != x] = 0\n",
    "    y = torch.tensor([classes.index(bin_name.loc[object_id].target)])\n",
    "    return x, y\n",
    "\n",
    "def random_data_point(bin_name):\n",
    "    object_id = bin_name.sample().index.values[0]\n",
    "    return get_data_point(object_id, bin_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size=1):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        #TODO add dropout or something\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,dropout=.2, num_layers=2)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        x = sequence.view(len(sequence), self.batch_size , -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        output  = self.hidden2out(lstm_out[-1])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(2, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(2, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = tuple(metadata_train.target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return classes[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, bin_name, sample=1):\n",
    "    labels = []\n",
    "    right = 0\n",
    "    s = bin_name.sample(frac=sample)\n",
    "    with torch.no_grad():\n",
    "        for obj_id in s.index:\n",
    "            x, y = get_data_point(obj_id, s)\n",
    "            y_hat = model(x)\n",
    "            label = categoryFromOutput(y_hat)[0]\n",
    "            labels.append([obj_id, label, bin_name.loc[obj_id].target])\n",
    "            if classes[y]==label:\n",
    "                right += 1\n",
    "    accuracy = right/len(s)\n",
    "    print(\"Accuracy: {:.2f}\".format(right/len(s)*100))\n",
    "    labels= pd.DataFrame(labels, columns=[\"object_id\", \"y_pred\", \"y_true\"])\n",
    "    labels.set_index(\"object_id\", inplace=True)\n",
    "    return labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample(bin_name, n=None):\n",
    "    from sklearn.utils import resample\n",
    "    classes = bin_name.target.unique()\n",
    "    class_dfs = [bin_name[bin_name.target == class_] for class_ in classes]\n",
    "    if not n:\n",
    "        n = np.max([len(df) for df in class_dfs])\n",
    "    even_dfs = [resample(df,\n",
    "                        replace=True,\n",
    "                        n_samples=n) for df in class_dfs]\n",
    "    return pd.concat(even_dfs).sample(frac=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_bin_name, valid_bin_name, n_samples=None, epochs=1, verbose=True):\n",
    "    \n",
    "    # Initialize Model\n",
    "    print(\"Building LSTM model..\")\n",
    "    model = RNN(6, 32, 14)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0005)\n",
    "    print(\"Done.\")\n",
    "    print_every = 100\n",
    "    current_loss = 0\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    max_acc = -1\n",
    "\n",
    "    start_t = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        randomized_bin = up_sample(train_bin_name, n=n_samples)\n",
    "        n = len(randomized_bin)\n",
    "        for i, row in randomized_bin.iterrows():\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of light curve inputs.\n",
    "            obj_id = row['object_id']\n",
    "            x_train, y_train = get_data_point(obj_id, train_bin_name)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            y_hat = model(x_train)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(y_hat, y_train)\n",
    "            current_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((i+1)%print_every == 0 and verbose):\n",
    "                print(\"Epoch {} {:.0f}% ===> Avg Loss: {:.3f}\".format(epoch+1, (i/n*100), current_loss/print_every))\n",
    "                current_loss=0\n",
    "                \n",
    "        # After each epoch test on training and validation data\n",
    "        print('Epoch {}: Training'.format(epoch+1), end = ' ')\n",
    "        _, tacc = predict(model, train_bin_name)\n",
    "        train_acc.append(tacc)\n",
    "        \n",
    "        print(\"         Validation\", end =\" \")\n",
    "        _, vacc = predict(model, valid_bin_name)\n",
    "        valid_acc.append(vacc)\n",
    "        if vacc > max_acc:\n",
    "            max_acc = vacc\n",
    "            print(f\"         Model achieved record validation accuracy, Saving to {start_t}_E{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), f'{start_t}_E{epoch}.pt')\n",
    "            print(\"         Done.\")\n",
    "            \n",
    "    print(f\"Finished {epochs} epochs in {time.time() - start_t}\")\n",
    "    return model, train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bf064a83a7b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split into validation and training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mddf_far_away_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddf_far_away_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddf_far_away\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mddf_nearby_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddf_nearby_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddf_nearby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnon_ddf_far_away_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_ddf_far_away_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_ddf_far_away\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split into validation and training.\n",
    "ddf_far_away_train, ddf_far_away_validation = train_test_split(ddf_far_away, test_size=0.1)\n",
    "ddf_nearby_train, ddf_nearby_validation = train_test_split(ddf_nearby, test_size=0.1)\n",
    "    \n",
    "non_ddf_far_away_train, non_ddf_far_away_validation = train_test_split(non_ddf_far_away, test_size=0.1)\n",
    "non_ddf_nearby_train, non_ddf_nearby_validation = train_test_split(non_ddf_nearby, test_size=0.1)\n",
    "    \n",
    "# Bin up\n",
    "training_bin = [ddf_far_away_train, ddf_nearby_train, \n",
    "                non_ddf_far_away_train, non_ddf_nearby_train]\n",
    "\n",
    "validation_bin = [ddf_far_away_validation, ddf_nearby_validation, \n",
    "                   non_ddf_far_away_validation, non_ddf_nearby_validation]\n",
    "\n",
    "# filenames = ['ddf_far_away', 'ddf_nearby', 'non_ddf_far_away', 'non_ddf_nearby']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddf_nearby_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-80fadd29ddd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddf_nearby_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddf_nearby_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ddf_nearby_train' is not defined"
     ]
    }
   ],
   "source": [
    "model = train(ddf_nearby_train, ddf_nearby_validation, epochs=25, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = [train(train_bin, valid_bin, epochs=25, verbose=False) for train_bin, valid_bin in zip(training_bin, validation_bin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pd.concat([predict(model[0], valid_bin)[0] for model, valid_bin in zip(models, validation_bin)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(labels, plot=True):\n",
    "    classes = np.sort(list(labels.y_true.unique()))\n",
    "    y_pred = np.array(labels.y_pred)\n",
    "    y_true = np.array(labels.y_true)\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(9,9))\n",
    "    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, cmap='inferno', annot=True, lw=1, fmt=\".2f\")\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "def plot_accuracy(train_acc, valid_acc):\n",
    "    fig_nodes = plt.figure(figsize=(8,8))\n",
    "    ax1 = fig_nodes.add_subplot(1,1,1)\n",
    "    ax1.plot(train_acc, marker='o', markersize=5, label = \"Training\")\n",
    "    ax1.plot(valid_acc, marker='+', markersize=5, label = \"Validation\")\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_title(f\"Training Accuracy wrt Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
