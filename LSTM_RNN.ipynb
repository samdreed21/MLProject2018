{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pd.read_csv('training_set.csv')\n",
    "metadata_train = pd.read_csv('training_set_metadata.csv')\n",
    "\n",
    "simple_features = train_series.groupby(\n",
    "    ['object_id', 'passband'])['flux'].agg(\n",
    "    ['mean', 'median', 'max', 'min', 'std']).unstack('passband')\n",
    "\n",
    "\n",
    "#construct time series using binned observations:\n",
    "ts_mod = train_series[['object_id', 'mjd', 'passband', 'flux']].copy()\n",
    "#bin by 5 days, reducing the size of data but still giving a time series\n",
    "ts_mod['mjd_d5'] = (ts_mod['mjd'] / 5).astype(int)\n",
    "ts_mod = ts_mod.groupby(['object_id', 'mjd_d5', 'passband'])['flux'].mean().reset_index()\n",
    "\n",
    "#pivotting\n",
    "ts_piv = pd.pivot_table(ts_mod, \n",
    "                        index='object_id', \n",
    "                        columns=['mjd_d5', 'passband'], \n",
    "                        values=['flux'],\n",
    "                        dropna=False)\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del metadata_train['ra'],metadata_train['decl'],metadata_train['gal_l'], metadata_train['gal_b'],metadata_train['hostgal_photoz'],metadata_train['hostgal_photoz_err'], metadata_train['distmod'], metadata_train['mwebv']\n",
    "#Bin into ddf and non-ddf training\n",
    "ddf = metadata_train[(metadata_train['ddf'] == 1)]\n",
    "del ddf['ddf']\n",
    "\n",
    "ddf_far_away= (ddf[(ddf['hostgal_specz'] > 0)])\n",
    "ddf_far_away.set_index('object_id', inplace=True)\n",
    "ddf_nearby= ddf[(ddf['hostgal_specz'] <=0)]\n",
    "ddf_nearby.set_index('object_id', inplace=True)\n",
    "non_ddf = metadata_train[(metadata_train['ddf'] == 0)]\n",
    "del non_ddf['ddf']\n",
    "\n",
    "non_ddf_far_away= non_ddf[(non_ddf['hostgal_specz'] >0)]\n",
    "non_ddf_far_away.set_index('object_id', inplace=True)\n",
    "non_ddf_nearby= non_ddf[(non_ddf['hostgal_specz'] <=0 )]\n",
    "non_ddf_nearby.set_index('object_id', inplace=True)\n",
    "del ddf, non_ddf, ddf_far_away['hostgal_specz'], non_ddf_far_away['hostgal_specz'], ddf_nearby['hostgal_specz'], non_ddf_nearby['hostgal_specz']\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "bins = [ddf_far_away, ddf_nearby, non_ddf_far_away, non_ddf_nearby]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_point(object_id, bin_name):\n",
    "    x = torch.tensor(ts_piv.loc[object_id].values.reshape(-1, 1, 6), dtype = torch.float32)\n",
    "    x[x != x] = 0\n",
    "    y = torch.tensor([classes.index(bin_name.loc[object_id].target)])\n",
    "    return x, y\n",
    "\n",
    "def random_data_point(bin_name):\n",
    "    object_id = bin_name.sample().index.values[0]\n",
    "    return get_data_point(object_id, bin_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size=1):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        #TODO add dropout or something\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size) #,dropout=.2, num_layers=2)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        x = sequence.view(len(sequence), self.batch_size , -1)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        output  = self.hidden2out(lstm_out[-1])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = tuple(metadata_train.target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return classes[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, bin_name, sample=1):\n",
    "    labels = []\n",
    "    right = 0\n",
    "    s = bin_name.sample(frac=sample)\n",
    "    with torch.no_grad():\n",
    "        for obj_id in s.index:\n",
    "            x, y = get_data_point(obj_id, s)\n",
    "            y_hat = model(x)\n",
    "            label = categoryFromOutput(y_hat)[0]\n",
    "            labels.append([obj_id, label, bin_name.loc[obj_id].target])\n",
    "            if classes[y]==label:\n",
    "                right += 1\n",
    "    accuracy = right/len(s)\n",
    "    print(\"Accuracy: {:.2f}\".format(right/len(s)*100))\n",
    "    labels= pd.DataFrame(labels, columns=[\"object_id\", \"y_pred\", \"y_true\"])\n",
    "    labels.set_index(\"object_id\", inplace=True)\n",
    "    return labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample(bin_name, n=None):\n",
    "    from sklearn.utils import resample\n",
    "    classes = bin_name.target.unique()\n",
    "    class_dfs = [bin_name[bin_name.target == class_] for class_ in classes]\n",
    "    if not n:\n",
    "        n = np.max([len(df) for df in class_dfs])\n",
    "    even_dfs = [resample(df,\n",
    "                        replace=True,\n",
    "                        n_samples=n) for df in class_dfs]\n",
    "    return pd.concat(even_dfs).sample(frac=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_bin_name, valid_bin_name, n_samples=None, epochs=1, verbose=True):\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = RNN(6, 48, 14)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.001)\n",
    "    \n",
    "    print_every = 100\n",
    "    current_loss = 0\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    start_t = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        randomized_bin = up_sample(train_bin_name, n=n_samples)\n",
    "        n = len(randomized_bin)\n",
    "        for i, row in randomized_bin.iterrows():\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of light curve inputs.\n",
    "            obj_id = row['object_id']\n",
    "            x_train, y_train = get_data_point(obj_id, train_bin_name)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            y_hat = model(x_train)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(y_hat, y_train)\n",
    "            current_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((i+1)%print_every == 0 and verbose):\n",
    "                print(\"Epoch {} {:.0f}% ===> Avg Loss: {:.3f}\".format(epoch+1, (i/n*100), current_loss/print_every))\n",
    "                current_loss=0\n",
    "                \n",
    "        # After each epoch test on training and validation data\n",
    "        print('Epoch {}: Training'.format(epoch+1), end = ' ')\n",
    "        _, tacc = predict(model, train_bin_name)\n",
    "        train_acc.append(tacc)\n",
    "        \n",
    "        print(\"         Validation\", end =\" \")\n",
    "        _, vacc = predict(model, valid_bin_name)\n",
    "        valid_acc.append(vacc)\n",
    "        \n",
    "    print(\"Finished\")\n",
    "    return model, train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into validation and training.\n",
    "ddf_far_away_train, ddf_far_away_validation = train_test_split(ddf_far_away, test_size=0.1)\n",
    "ddf_nearby_train, ddf_nearby_validation = train_test_split(ddf_nearby, test_size=0.1)\n",
    "    \n",
    "non_ddf_far_away_train, non_ddf_far_away_validation = train_test_split(non_ddf_far_away, test_size=0.1)\n",
    "non_ddf_nearby_train, non_ddf_nearby_validation = train_test_split(non_ddf_nearby, test_size=0.1)\n",
    "    \n",
    "# Bin up\n",
    "training_bin = [ddf_far_away_train, ddf_nearby_train, \n",
    "                non_ddf_far_away_train, non_ddf_nearby_train]\n",
    "\n",
    "validation_bin = [ddf_far_away_validation, ddf_nearby_validation, \n",
    "                   non_ddf_far_away_validation, non_ddf_nearby_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 11% ===> Avg Loss: 2.540\n",
      "Epoch 1 22% ===> Avg Loss: 2.397\n",
      "Epoch 1 33% ===> Avg Loss: 2.321\n",
      "Epoch 1 44% ===> Avg Loss: 2.301\n",
      "Epoch 1 55% ===> Avg Loss: 2.297\n",
      "Epoch 1 67% ===> Avg Loss: 2.246\n",
      "Epoch 1 78% ===> Avg Loss: 2.197\n",
      "Epoch 1 89% ===> Avg Loss: 2.242\n",
      "Epoch 1 100% ===> Avg Loss: 2.155\n",
      "Epoch 1: Training Accuracy: 14.88\n",
      "         Validation Accuracy: 17.72\n",
      "Epoch 2 11% ===> Avg Loss: 2.179\n",
      "Epoch 2 22% ===> Avg Loss: 2.122\n",
      "Epoch 2 33% ===> Avg Loss: 2.078\n",
      "Epoch 2 44% ===> Avg Loss: 2.180\n",
      "Epoch 2 55% ===> Avg Loss: 2.083\n",
      "Epoch 2 67% ===> Avg Loss: 2.166\n",
      "Epoch 2 78% ===> Avg Loss: 2.029\n",
      "Epoch 2 89% ===> Avg Loss: 2.086\n",
      "Epoch 2 100% ===> Avg Loss: 2.184\n",
      "Epoch 2: Training Accuracy: 5.92\n",
      "         Validation Accuracy: 6.33\n",
      "Epoch 3 11% ===> Avg Loss: 2.147\n",
      "Epoch 3 22% ===> Avg Loss: 2.161\n",
      "Epoch 3 33% ===> Avg Loss: 2.126\n",
      "Epoch 3 44% ===> Avg Loss: 1.994\n",
      "Epoch 3 55% ===> Avg Loss: 2.127\n",
      "Epoch 3 67% ===> Avg Loss: 2.065\n",
      "Epoch 3 78% ===> Avg Loss: 2.089\n",
      "Epoch 3 89% ===> Avg Loss: 2.021\n",
      "Epoch 3 100% ===> Avg Loss: 2.022\n",
      "Epoch 3: Training Accuracy: 18.34\n",
      "         Validation Accuracy: 17.09\n",
      "Epoch 4 11% ===> Avg Loss: 2.109\n",
      "Epoch 4 22% ===> Avg Loss: 1.989\n",
      "Epoch 4 33% ===> Avg Loss: 2.024\n",
      "Epoch 4 44% ===> Avg Loss: 2.054\n",
      "Epoch 4 55% ===> Avg Loss: 2.194\n",
      "Epoch 4 67% ===> Avg Loss: 2.173\n",
      "Epoch 4 78% ===> Avg Loss: 2.065\n",
      "Epoch 4 89% ===> Avg Loss: 1.942\n",
      "Epoch 4 100% ===> Avg Loss: 2.084\n",
      "Epoch 4: Training Accuracy: 6.84\n",
      "         Validation Accuracy: 5.70\n",
      "Epoch 5 11% ===> Avg Loss: 2.020\n",
      "Epoch 5 22% ===> Avg Loss: 1.951\n",
      "Epoch 5 33% ===> Avg Loss: 1.855\n",
      "Epoch 5 44% ===> Avg Loss: 1.837\n",
      "Epoch 5 55% ===> Avg Loss: 1.842\n",
      "Epoch 5 67% ===> Avg Loss: 1.886\n",
      "Epoch 5 78% ===> Avg Loss: 1.915\n",
      "Epoch 5 89% ===> Avg Loss: 1.750\n",
      "Epoch 5 100% ===> Avg Loss: 2.105\n",
      "Epoch 5: Training Accuracy: 6.21\n",
      "         Validation Accuracy: 7.59\n",
      "Epoch 6 11% ===> Avg Loss: 2.097\n",
      "Epoch 6 22% ===> Avg Loss: 2.023\n",
      "Epoch 6 33% ===> Avg Loss: 2.129\n",
      "Epoch 6 44% ===> Avg Loss: 1.985\n",
      "Epoch 6 55% ===> Avg Loss: 1.979\n",
      "Epoch 6 67% ===> Avg Loss: 1.876\n",
      "Epoch 6 78% ===> Avg Loss: 1.896\n",
      "Epoch 6 89% ===> Avg Loss: 1.822\n",
      "Epoch 6 100% ===> Avg Loss: 1.935\n",
      "Epoch 6: Training Accuracy: 9.24\n",
      "         Validation Accuracy: 10.76\n",
      "Epoch 7 11% ===> Avg Loss: 1.963\n",
      "Epoch 7 22% ===> Avg Loss: 2.045\n",
      "Epoch 7 33% ===> Avg Loss: 2.134\n",
      "Epoch 7 44% ===> Avg Loss: 1.890\n",
      "Epoch 7 55% ===> Avg Loss: 1.856\n",
      "Epoch 7 67% ===> Avg Loss: 1.820\n",
      "Epoch 7 78% ===> Avg Loss: 1.849\n",
      "Epoch 7 89% ===> Avg Loss: 1.722\n",
      "Epoch 7 100% ===> Avg Loss: 1.916\n",
      "Epoch 7: Training Accuracy: 39.49\n",
      "         Validation Accuracy: 43.04\n",
      "Epoch 8 11% ===> Avg Loss: 1.771\n",
      "Epoch 8 22% ===> Avg Loss: 1.795\n",
      "Epoch 8 33% ===> Avg Loss: 1.874\n",
      "Epoch 8 44% ===> Avg Loss: 2.054\n",
      "Epoch 8 55% ===> Avg Loss: 1.901\n",
      "Epoch 8 67% ===> Avg Loss: 1.963\n",
      "Epoch 8 78% ===> Avg Loss: 1.862\n",
      "Epoch 8 89% ===> Avg Loss: 2.056\n",
      "Epoch 8 100% ===> Avg Loss: 2.067\n",
      "Epoch 8: Training Accuracy: 5.15\n",
      "         Validation Accuracy: 4.43\n",
      "Epoch 9 11% ===> Avg Loss: 1.997\n",
      "Epoch 9 22% ===> Avg Loss: 1.881\n",
      "Epoch 9 33% ===> Avg Loss: 1.950\n",
      "Epoch 9 44% ===> Avg Loss: 1.823\n",
      "Epoch 9 55% ===> Avg Loss: 1.835\n",
      "Epoch 9 67% ===> Avg Loss: 1.890\n",
      "Epoch 9 78% ===> Avg Loss: 1.585\n",
      "Epoch 9 89% ===> Avg Loss: 2.169\n",
      "Epoch 9 100% ===> Avg Loss: 1.872\n",
      "Epoch 9: Training Accuracy: 11.85\n",
      "         Validation Accuracy: 10.13\n",
      "Epoch 10 11% ===> Avg Loss: 1.655\n",
      "Epoch 10 22% ===> Avg Loss: 1.626\n",
      "Epoch 10 33% ===> Avg Loss: 1.606\n",
      "Epoch 10 44% ===> Avg Loss: 2.072\n",
      "Epoch 10 55% ===> Avg Loss: 1.916\n",
      "Epoch 10 67% ===> Avg Loss: 1.889\n",
      "Epoch 10 78% ===> Avg Loss: 1.678\n",
      "Epoch 10 89% ===> Avg Loss: 1.539\n"
     ]
    }
   ],
   "source": [
    "models = [train(train_bin, valid_bin, epochs=10, n_samples=100) for train_bin, valid_bin in zip(training_bin, validation_bin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pd.concat([predict(model[0], valid_bin)[0] for model, valid_bin in zip(models, validation_bin)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(labels, plot=True):\n",
    "    classes = np.sort(list(labels.y_true.unique()))\n",
    "    y_pred = np.array(labels.y_pred)\n",
    "    y_true = np.array(labels.y_true)\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(9,9))\n",
    "    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, cmap='inferno', annot=True, lw=1, fmt=\".2f\")\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "def plot_accuracy(train_acc, valid_acc):\n",
    "    fig_nodes = plt.figure(figsize=(8,8))\n",
    "    ax1 = fig_nodes.add_subplot(1,1,1)\n",
    "    ax1.plot(train_acc, marker='o', markersize=5, label = \"Training\")\n",
    "    ax1.plot(valid_acc, marker='+', markersize=5, label = \"Validation\")\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_title(f\"Training Accuracy wrt Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
