{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pd.read_csv('training_set.csv')\n",
    "metadata_train = pd.read_csv('training_set_metadata.csv')\n",
    "\n",
    "simple_features = train_series.groupby(\n",
    "    ['object_id', 'passband'])['flux'].agg(\n",
    "    ['mean', 'median', 'max', 'min', 'std']).unstack('passband')\n",
    "\n",
    "\n",
    "#construct time series using binned observations:\n",
    "ts_mod = train_series[['object_id', 'mjd', 'passband', 'flux']].copy()\n",
    "#bin by 5 days, reducing the size of data but still giving a time series\n",
    "ts_mod['mjd_d5'] = (ts_mod['mjd'] / 5).astype(int)\n",
    "ts_mod = ts_mod.groupby(['object_id', 'mjd_d5', 'passband'])['flux'].mean().reset_index()\n",
    "\n",
    "#pivotting\n",
    "ts_piv = pd.pivot_table(ts_mod, \n",
    "                        index='object_id', \n",
    "                        columns=['mjd_d5', 'passband'], \n",
    "                        values=['flux'],\n",
    "                        dropna=False)\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del metadata_train['ra'],metadata_train['decl'],metadata_train['gal_l'], metadata_train['gal_b'],metadata_train['hostgal_photoz'],metadata_train['hostgal_photoz_err'], metadata_train['distmod'], metadata_train['mwebv']\n",
    "#Bin into ddf and non-ddf training\n",
    "ddf = metadata_train[(metadata_train['ddf'] == 1)]\n",
    "del ddf['ddf']\n",
    "\n",
    "ddf_far_away= (ddf[(ddf['hostgal_specz'] > 0)])\n",
    "ddf_far_away.set_index('object_id', inplace=True)\n",
    "ddf_nearby= ddf[(ddf['hostgal_specz'] <=0)]\n",
    "ddf_nearby.set_index('object_id', inplace=True)\n",
    "non_ddf = metadata_train[(metadata_train['ddf'] == 0)]\n",
    "del non_ddf['ddf']\n",
    "\n",
    "non_ddf_far_away= non_ddf[(non_ddf['hostgal_specz'] >0)]\n",
    "non_ddf_far_away.set_index('object_id', inplace=True)\n",
    "non_ddf_nearby= non_ddf[(non_ddf['hostgal_specz'] <=0 )]\n",
    "non_ddf_nearby.set_index('object_id', inplace=True)\n",
    "del ddf, non_ddf, ddf_far_away['hostgal_specz'], non_ddf_far_away['hostgal_specz'], ddf_nearby['hostgal_specz'], non_ddf_nearby['hostgal_specz']\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "bins = [ddf_far_away, ddf_nearby, non_ddf_far_away, non_ddf_nearby]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_point(object_id, bin_name):\n",
    "    x = torch.tensor(ts_piv.loc[object_id].values.reshape(-1, 1, 6), dtype = torch.float32)\n",
    "    x[x != x] = 0\n",
    "    y = torch.tensor([classes.index(bin_name.loc[object_id].target)])\n",
    "    return x, y\n",
    "\n",
    "def random_data_point(bin_name):\n",
    "    object_id = bin_name.sample().index.values[0]\n",
    "    return get_data_point(object_id, bin_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size=1):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        #TODO add dropout or something\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size) #,dropout=.2, num_layers=2)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        x = sequence.view(len(sequence), self.batch_size , -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        output  = self.hidden2out(lstm_out[-1])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9364, 0.7748, 0.6260, 0.0000, 0.0000, 0.9640, 0.9661, 0.9101,\n",
       "        0.8766, 0.9714, 0.0000, 0.9747, 0.0000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = tuple(metadata_train.target.unique())\n",
    "vc = non_ddf_far_away.target.value_counts(normalize=True)\n",
    "weights = torch.tensor([1 - vc.loc[i] if i in vc.index else 0 for i in classes])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return classes[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(bin_name, epochs=1):\n",
    "    # Calculate weights of each class to balance\n",
    "    vc = bin_name.target.value_counts(normalize=True)\n",
    "    weights = torch.tensor([1 - vc.loc[i] if i in vc.index else 0 for i in classes])\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = RNN(6, 32, 14)\n",
    "    criterion = nn.NLLLoss(weight = weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.05, nesterov=True)\n",
    "    \n",
    "    n = len(bin_name)\n",
    "    print_every = 100\n",
    "    current_loss = 0\n",
    "    \n",
    "    start_t = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        randomized_bin = bin_name.sample(frac=1)\n",
    "        for i, obj_id in enumerate(randomized_bin.index):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of light curve inputs.\n",
    "            x_train, y_train = get_data_point(obj_id, randomized_bin)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            y_hat = model(x_train)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(y_hat, y_train)\n",
    "            current_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i % print_every == 0):\n",
    "                print(\"Epoch {} {:.0f}% ===> Avg Loss: {:.3f}\".format(epoch+1, (i/n*100), current_loss/print_every))\n",
    "                current_loss=0\n",
    "        predict(model, bin_name, sample=.5)\n",
    "    print(\"Finished\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, bin_name, sample=1):\n",
    "    labels = []\n",
    "    right = 0\n",
    "    s = bin_name.sample(frac=sample)\n",
    "    with torch.no_grad():\n",
    "        for obj_id in s.index:\n",
    "            x, y = get_data_point(obj_id, s)\n",
    "            y_hat = model(x)\n",
    "            label = categoryFromOutput(y_hat)[0]\n",
    "            labels.append([obj_id, label])\n",
    "            if classes[y]==label:\n",
    "                right += 1\n",
    "    print(f\"Accuracy: {right/len(s)*100}\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 0% ===> Avg Loss: 0.026\n",
      "Epoch 1 6% ===> Avg Loss: 1.928\n",
      "Epoch 1 13% ===> Avg Loss: 1.616\n",
      "Epoch 1 19% ===> Avg Loss: 1.452\n",
      "Epoch 1 25% ===> Avg Loss: 1.427\n",
      "Epoch 1 32% ===> Avg Loss: 1.382\n",
      "Epoch 1 38% ===> Avg Loss: 1.663\n",
      "Epoch 1 44% ===> Avg Loss: 1.474\n",
      "Epoch 1 51% ===> Avg Loss: 1.667\n",
      "Epoch 1 57% ===> Avg Loss: 1.527\n",
      "Epoch 1 63% ===> Avg Loss: 1.466\n",
      "Epoch 1 70% ===> Avg Loss: 1.477\n",
      "Epoch 1 76% ===> Avg Loss: 1.539\n",
      "Epoch 1 82% ===> Avg Loss: 1.481\n",
      "Epoch 1 89% ===> Avg Loss: 1.547\n",
      "Epoch 1 95% ===> Avg Loss: 1.585\n",
      "Accuracy: 21.5100076007094\n",
      "Epoch 2 0% ===> Avg Loss: 1.090\n",
      "Epoch 2 6% ===> Avg Loss: 1.712\n",
      "Epoch 2 13% ===> Avg Loss: 1.453\n",
      "Epoch 2 19% ===> Avg Loss: 1.430\n",
      "Epoch 2 25% ===> Avg Loss: 1.376\n",
      "Epoch 2 32% ===> Avg Loss: 1.540\n",
      "Epoch 2 38% ===> Avg Loss: 1.491\n",
      "Epoch 2 44% ===> Avg Loss: 1.522\n",
      "Epoch 2 51% ===> Avg Loss: 1.639\n",
      "Epoch 2 57% ===> Avg Loss: 1.266\n",
      "Epoch 2 63% ===> Avg Loss: 1.397\n",
      "Epoch 2 70% ===> Avg Loss: 1.465\n",
      "Epoch 2 76% ===> Avg Loss: 1.339\n",
      "Epoch 2 82% ===> Avg Loss: 1.313\n",
      "Epoch 2 89% ===> Avg Loss: 1.374\n",
      "Epoch 2 95% ===> Avg Loss: 1.535\n",
      "Accuracy: 22.270078540663796\n",
      "Epoch 3 0% ===> Avg Loss: 1.030\n",
      "Epoch 3 6% ===> Avg Loss: 1.372\n",
      "Epoch 3 13% ===> Avg Loss: 1.067\n",
      "Epoch 3 19% ===> Avg Loss: 1.272\n",
      "Epoch 3 25% ===> Avg Loss: 1.441\n",
      "Epoch 3 32% ===> Avg Loss: 1.361\n",
      "Epoch 3 38% ===> Avg Loss: 1.601\n",
      "Epoch 3 44% ===> Avg Loss: 1.380\n",
      "Epoch 3 51% ===> Avg Loss: 1.530\n",
      "Epoch 3 57% ===> Avg Loss: 1.726\n",
      "Epoch 3 63% ===> Avg Loss: 1.676\n",
      "Epoch 3 70% ===> Avg Loss: 1.441\n",
      "Epoch 3 76% ===> Avg Loss: 1.463\n",
      "Epoch 3 82% ===> Avg Loss: 1.407\n",
      "Epoch 3 89% ===> Avg Loss: 1.412\n",
      "Epoch 3 95% ===> Avg Loss: 1.431\n",
      "Accuracy: 21.687357486698758\n",
      "Epoch 4 0% ===> Avg Loss: 1.156\n",
      "Epoch 4 6% ===> Avg Loss: 1.505\n",
      "Epoch 4 13% ===> Avg Loss: 1.462\n",
      "Epoch 4 19% ===> Avg Loss: 1.476\n",
      "Epoch 4 25% ===> Avg Loss: 1.437\n",
      "Epoch 4 32% ===> Avg Loss: 1.505\n",
      "Epoch 4 38% ===> Avg Loss: 1.492\n",
      "Epoch 4 44% ===> Avg Loss: 1.286\n",
      "Epoch 4 51% ===> Avg Loss: 1.298\n",
      "Epoch 4 57% ===> Avg Loss: 1.407\n",
      "Epoch 4 63% ===> Avg Loss: 1.405\n",
      "Epoch 4 70% ===> Avg Loss: 1.291\n",
      "Epoch 4 76% ===> Avg Loss: 1.342\n",
      "Epoch 4 82% ===> Avg Loss: 1.382\n",
      "Epoch 4 89% ===> Avg Loss: 1.361\n",
      "Epoch 4 95% ===> Avg Loss: 1.375\n",
      "Accuracy: 22.396757030656193\n",
      "Epoch 5 0% ===> Avg Loss: 0.866\n",
      "Epoch 5 6% ===> Avg Loss: 1.281\n",
      "Epoch 5 13% ===> Avg Loss: 1.185\n",
      "Epoch 5 19% ===> Avg Loss: 1.236\n",
      "Epoch 5 25% ===> Avg Loss: 1.502\n",
      "Epoch 5 32% ===> Avg Loss: 1.402\n",
      "Epoch 5 38% ===> Avg Loss: 1.409\n",
      "Epoch 5 44% ===> Avg Loss: 1.363\n",
      "Epoch 5 51% ===> Avg Loss: 1.393\n",
      "Epoch 5 57% ===> Avg Loss: 1.291\n",
      "Epoch 5 63% ===> Avg Loss: 1.386\n",
      "Epoch 5 70% ===> Avg Loss: 1.565\n",
      "Epoch 5 76% ===> Avg Loss: 1.319\n",
      "Epoch 5 82% ===> Avg Loss: 1.277\n",
      "Epoch 5 89% ===> Avg Loss: 1.323\n",
      "Epoch 5 95% ===> Avg Loss: 1.161\n",
      "Accuracy: 23.891563212566506\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "model = train(ddf_far_away, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.83502538071066\n"
     ]
    }
   ],
   "source": [
    "y_hat = predict(model, ddf_far_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
